{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1\n",
      "Train Loss: 3.308, Train Acc: 49.73%\n",
      "Test Loss: 6.006, Test Acc: 49.94%\n",
      "Train Loss: 3.308, Train Acc: 49.73%\n",
      "Test Loss: 6.006, Test Acc: 49.94%\n",
      "Epoch 2\n",
      "Train Loss: 2.405, Train Acc: 52.90%\n",
      "Test Loss: 2.875, Test Acc: 50.85%\n",
      "Train Loss: 2.405, Train Acc: 52.90%\n",
      "Test Loss: 2.875, Test Acc: 50.85%\n",
      "Epoch 3\n",
      "Train Loss: 2.160, Train Acc: 53.84%\n",
      "Test Loss: 2.656, Test Acc: 51.81%\n",
      "Train Loss: 2.160, Train Acc: 53.84%\n",
      "Test Loss: 2.656, Test Acc: 51.81%\n",
      "Epoch 4\n",
      "Train Loss: 1.045, Train Acc: 60.79%\n",
      "Test Loss: 1.376, Test Acc: 58.77%\n",
      "Train Loss: 1.045, Train Acc: 60.79%\n",
      "Test Loss: 1.376, Test Acc: 58.77%\n",
      "Epoch 5\n",
      "Train Loss: 0.901, Train Acc: 70.74%\n",
      "Test Loss: 0.989, Test Acc: 66.71%\n",
      "Train Loss: 0.901, Train Acc: 70.74%\n",
      "Test Loss: 0.989, Test Acc: 66.71%\n",
      "Epoch 6\n",
      "Train Loss: 0.701, Train Acc: 74.73%\n",
      "Test Loss: 0.889, Test Acc: 70.91%\n",
      "Train Loss: 0.701, Train Acc: 74.73%\n",
      "Test Loss: 0.889, Test Acc: 70.91%\n",
      "Epoch 7\n",
      "Train Loss: 0.679, Train Acc: 77.23%\n",
      "Test Loss: 0.756, Test Acc: 77.85%\n",
      "Train Loss: 0.679, Train Acc: 77.23%\n",
      "Test Loss: 0.756, Test Acc: 77.85%\n",
      "Epoch 8\n",
      "Train Loss: 0.523, Train Acc: 78.51%\n",
      "Test Loss: 0.611, Test Acc: 78.92%\n",
      "Train Loss: 0.523, Train Acc: 78.51%\n",
      "Test Loss: 0.611, Test Acc: 78.92%\n",
      "Epoch 9\n",
      "Train Loss: 0.389, Train Acc: 79.63%\n",
      "Test Loss: 0.457, Test Acc: 79.41%\n",
      "Train Loss: 0.389, Train Acc: 79.63%\n",
      "Test Loss: 0.457, Test Acc: 79.41%\n",
      "Epoch 10\n",
      "Train Loss: 0.498, Train Acc: 80.27%\n",
      "Test Loss: 0.374, Test Acc: 80.56%\n",
      "Train Loss: 0.498, Train Acc: 80.27%\n",
      "Test Loss: 0.374, Test Acc: 80.56%\n",
      "Epoch 11\n",
      "Train Loss: 0.260, Train Acc: 81.41%\n",
      "Test Loss: 0.312, Test Acc: 81.73%\n",
      "Train Loss: 0.260, Train Acc: 81.41%\n",
      "Test Loss: 0.312, Test Acc: 81.73%\n",
      "Epoch 12\n",
      "Train Loss: 0.278, Train Acc: 82.12%\n",
      "Test Loss: 0.306, Test Acc: 82.37%\n",
      "Train Loss: 0.278, Train Acc: 82.12%\n",
      "Test Loss: 0.306, Test Acc: 82.37%\n",
      "Epoch 13\n",
      "Train Loss: 0.155, Train Acc: 83.07%\n",
      "Test Loss: 0.231, Test Acc: 83.29%\n",
      "Train Loss: 0.155, Train Acc: 83.07%\n",
      "Test Loss: 0.231, Test Acc: 83.29%\n",
      "Epoch 14\n",
      "Train Loss: 0.094, Train Acc: 84.09%\n",
      "Test Loss: 0.129, Test Acc: 84.18%\n",
      "Train Loss: 0.094, Train Acc: 84.09%\n",
      "Test Loss: 0.129, Test Acc: 84.18%\n",
      "Epoch 15\n",
      "Train Loss: 0.089, Train Acc: 85.32%\n",
      "Test Loss: 0.145, Test Acc: 85.14%\n",
      "Train Loss: 0.089, Train Acc: 85.32%\n",
      "Test Loss: 0.145, Test Acc: 85.14%\n",
      "Epoch 16\n",
      "Train Loss: 0.112, Train Acc: 86.47%\n",
      "Test Loss: 0.132, Test Acc: 86.28%\n",
      "Train Loss: 0.112, Train Acc: 86.47%\n",
      "Test Loss: 0.132, Test Acc: 86.28%\n",
      "Epoch 17\n",
      "Train Loss: 0.074, Train Acc: 87.59%\n",
      "Test Loss: 0.102, Test Acc: 87.03%\n",
      "Train Loss: 0.074, Train Acc: 87.59%\n",
      "Test Loss: 0.102, Test Acc: 87.03%\n",
      "Epoch 18\n",
      "Train Loss: 0.062, Train Acc: 88.91%\n",
      "Test Loss: 0.093, Test Acc: 88.44%\n",
      "Train Loss: 0.062, Train Acc: 88.91%\n",
      "Test Loss: 0.093, Test Acc: 88.441%\n",
      "Epoch 19\n",
      "Train Loss: 0.057, Train Acc: 92.34%\n",
      "Test Loss: 0.089, Test Acc: 91.72%\n",
      "Train Loss: 0.057, Train Acc: 92.34%\n",
      "Test Loss: 0.089, Test Acc: 91.72%\n",
      "Epoch 20\n",
      "Train Loss: 0.075, Train Acc: 90.28%\n",
      "Test Loss: 0.107, Test Acc: 89.26%\n",
      "Train Loss: 0.075, Train Acc: 90.28%\n",
      "Test Loss: 0.107, Test Acc: 89.26%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# 数据预处理\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "def tokenizer(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, include_lengths=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=True, is_target=True, batch_first=True)\n",
    "\n",
    "# 下载IMDB数据集\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 构建词汇表\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# 数据加载器\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")\n",
    "\n",
    "# 定义RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        # 循环层（RNN）\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc.weight)  # Xavier初始化\n",
    "        nn.init.zeros_(self.fc.bias)  # 偏置初始化为0\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text是一个batch的输入，text_lengths是每个句子的实际长度\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # pack_padded_sequence 用来处理不同长度的序列\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # 通过RNN层\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # 通过Dropout层\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        \n",
    "        # 输出\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "# 设置模型超参数\n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# 实例化模型并将其转移到相应的设备\n",
    "model = RNNModel(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# 使用预训练的GloVe嵌入\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "# 初始化模型的输出层权重\n",
    "model.fc.weight.data.normal_(0, 0.01)\n",
    "model.fc.bias.data.fill_(0)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # 可调整学习率\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 训练函数\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label.float()\n",
    "        \n",
    "        # 将数据移到设备\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        \n",
    "        # 将 text_lengths 移到 CPU\n",
    "        text_lengths = text_lengths.to('cpu')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "# 准确率计算函数\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))  # 直接计算预测的标签\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            labels = batch.label.float()\n",
    "            \n",
    "            # 将数据移到设备\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            # 将 text_lengths 移到 CPU\n",
    "            text_lengths = text_lengths.to('cpu')\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# 训练和评估模型\n",
    "N_EPOCHS = 20  # 增加 epoch 数量\n",
    "print(\"start training\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
