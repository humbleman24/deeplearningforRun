{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1\n",
      "Train Loss: -3.308, Train Acc: 49.73%\n",
      "Test Loss: -6.006, Test Acc: 49.94%\n",
      "Train Loss: -3.308, Train Acc: 49.73%\n",
      "Test Loss: -6.006, Test Acc: 49.94%\n",
      "Epoch 2\n",
      "Train Loss: -8.405, Train Acc: 49.90%\n",
      "Test Loss: -10.875, Test Acc: 49.85%\n",
      "Train Loss: -8.405, Train Acc: 49.90%\n",
      "Test Loss: -10.875, Test Acc: 49.85%\n",
      "Epoch 3\n",
      "Train Loss: -13.160, Train Acc: 49.84%\n",
      "Test Loss: -15.656, Test Acc: 49.81%\n",
      "Train Loss: -13.160, Train Acc: 49.84%\n",
      "Test Loss: -15.656, Test Acc: 49.81%\n",
      "Epoch 4\n",
      "Train Loss: -17.845, Train Acc: 49.79%\n",
      "Test Loss: -20.376, Test Acc: 49.77%\n",
      "Train Loss: -17.845, Train Acc: 49.79%\n",
      "Test Loss: -20.376, Test Acc: 49.77%\n",
      "Epoch 5\n",
      "Train Loss: -22.401, Train Acc: 49.74%\n",
      "Test Loss: -24.989, Test Acc: 49.71%\n",
      "Train Loss: -22.401, Train Acc: 49.74%\n",
      "Test Loss: -24.989, Test Acc: 49.71%\n",
      "Epoch 6\n",
      "Train Loss: -26.982, Train Acc: 49.69%\n",
      "Test Loss: -29.632, Test Acc: 49.70%\n",
      "Train Loss: -26.982, Train Acc: 49.69%\n",
      "Test Loss: -29.632, Test Acc: 49.70%\n",
      "Epoch 7\n",
      "Train Loss: -31.539, Train Acc: 49.67%\n",
      "Test Loss: -34.230, Test Acc: 49.66%\n",
      "Train Loss: -31.539, Train Acc: 49.67%\n",
      "Test Loss: -34.230, Test Acc: 49.66%\n",
      "Epoch 8\n",
      "Train Loss: -36.063, Train Acc: 49.63%\n",
      "Test Loss: -38.779, Test Acc: 49.64%\n",
      "Train Loss: -36.063, Train Acc: 49.63%\n",
      "Test Loss: -38.779, Test Acc: 49.64%\n",
      "Epoch 9\n",
      "Train Loss: -40.508, Train Acc: 49.61%\n",
      "Test Loss: -43.391, Test Acc: 49.61%\n",
      "Train Loss: -40.508, Train Acc: 49.61%\n",
      "Test Loss: -43.391, Test Acc: 49.61%\n",
      "Epoch 10\n",
      "Train Loss: -44.902, Train Acc: 49.58%\n",
      "Test Loss: -47.998, Test Acc: 49.60%\n",
      "Train Loss: -44.902, Train Acc: 49.58%\n",
      "Test Loss: -47.998, Test Acc: 49.60%\n",
      "Epoch 11\n",
      "Train Loss: -49.371, Train Acc: 49.55%\n",
      "Test Loss: -52.522, Test Acc: 49.58%\n",
      "Train Loss: -49.371, Train Acc: 49.55%\n",
      "Test Loss: -52.522, Test Acc: 49.58%\n",
      "Epoch 12\n",
      "Train Loss: -53.732, Train Acc: 49.51%\n",
      "Test Loss: -57.066, Test Acc: 49.56%\n",
      "Train Loss: -53.732, Train Acc: 49.51%\n",
      "Test Loss: -57.066, Test Acc: 49.56%\n",
      "Epoch 13\n",
      "Train Loss: -58.183, Train Acc: 49.50%\n",
      "Test Loss: -61.560, Test Acc: 49.52%\n",
      "Train Loss: -58.183, Train Acc: 49.50%\n",
      "Test Loss: -61.560, Test Acc: 49.52%\n",
      "Epoch 14\n",
      "Train Loss: -62.604, Train Acc: 49.46%\n",
      "Test Loss: -65.939, Test Acc: 49.48%\n",
      "Train Loss: -62.604, Train Acc: 49.46%\n",
      "Test Loss: -65.939, Test Acc: 49.48%\n",
      "Epoch 15\n",
      "Train Loss: -66.886, Train Acc: 49.42%\n",
      "Test Loss: -70.340, Test Acc: 49.46%\n",
      "Train Loss: -66.886, Train Acc: 49.42%\n",
      "Test Loss: -70.340, Test Acc: 49.46%\n",
      "Epoch 16\n",
      "Train Loss: -71.270, Train Acc: 49.41%\n",
      "Test Loss: -74.735, Test Acc: 49.43%\n",
      "Train Loss: -71.270, Train Acc: 49.41%\n",
      "Test Loss: -74.735, Test Acc: 49.43%\n",
      "Epoch 17\n",
      "Train Loss: -75.582, Train Acc: 49.36%\n",
      "Test Loss: -79.206, Test Acc: 49.41%\n",
      "Train Loss: -75.582, Train Acc: 49.36%\n",
      "Test Loss: -79.206, Test Acc: 49.41%\n",
      "Epoch 18\n",
      "Train Loss: -79.802, Train Acc: 49.36%\n",
      "Test Loss: -83.551, Test Acc: 49.39%\n",
      "Train Loss: -79.802, Train Acc: 49.36%\n",
      "Test Loss: -83.551, Test Acc: 49.39%\n",
      "Epoch 19\n",
      "Train Loss: -84.122, Train Acc: 49.35%\n",
      "Test Loss: -87.939, Test Acc: 49.36%\n",
      "Train Loss: -84.122, Train Acc: 49.35%\n",
      "Test Loss: -87.939, Test Acc: 49.36%\n",
      "Epoch 20\n",
      "Train Loss: -88.292, Train Acc: 49.33%\n",
      "Test Loss: -92.208, Test Acc: 49.35%\n",
      "Train Loss: -88.292, Train Acc: 49.33%\n",
      "Test Loss: -92.208, Test Acc: 49.35%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# 数据预处理\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "def tokenizer(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, include_lengths=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=True, is_target=True, batch_first=True)\n",
    "\n",
    "# 下载IMDB数据集\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 构建词汇表\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# 数据加载器\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")\n",
    "\n",
    "# 定义RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        # 循环层（RNN）\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text是一个batch的输入，text_lengths是每个句子的实际长度\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # pack_padded_sequence 用来处理不同长度的序列\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # 通过RNN层\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # 通过Dropout层\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        \n",
    "        # 输出\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "# 设置模型超参数\n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# 实例化模型并将其转移到相应的设备\n",
    "model = RNNModel(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# 使用预训练的GloVe嵌入\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "# 初始化模型的输出层权重\n",
    "model.fc.weight.data.normal_(0, 0.01)\n",
    "model.fc.bias.data.fill_(0)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # 可调整学习率\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 训练函数\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label.float()\n",
    "        \n",
    "        # 将数据移到设备\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        \n",
    "        # 将 text_lengths 移到 CPU\n",
    "        text_lengths = text_lengths.to('cpu')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "# 准确率计算函数\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))  # 直接计算预测的标签\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            labels = batch.label.float()\n",
    "            \n",
    "            # 将数据移到设备\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            # 将 text_lengths 移到 CPU\n",
    "            text_lengths = text_lengths.to('cpu')\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# 训练和评估模型\n",
    "N_EPOCHS = 20  # 增加 epoch 数量\n",
    "print(\"start training\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
